{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://open.spotify.com/playlist/37i9dQZF1DWXVJK4aT7pmk?si=03ef9c7e351b43f3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Spotify's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_credentials_manager = SpotifyClientCredentials(client_id='ac1f41b0f93c45ac8af29623bdf94e0a', client_secret='f2460ff1a77e4c6db5eca633dde46d3b')\n",
    "sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)\n",
    "\n",
    "playlist_link = input(\"Enter the link: \")\n",
    "\n",
    "playlist_URI = playlist_link.split(\"/\")[-1].split(\"?\")[0]\n",
    "\n",
    "songs = sp.playlist_items(playlist_URI, market='IN')['items']  #will give a list of songs (meta-data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the song meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_name = []\n",
    "song_id = []\n",
    "song_popu = []\n",
    "song_added_date = []\n",
    "song_release_date = []\n",
    "artists_col = []\n",
    "for song in songs:\n",
    "    song_name.append(song['track']['name'])\n",
    "    song_id.append(song['track']['id'])\n",
    "    song_popu.append(song['track']['popularity'])\n",
    "    song_added_date.append(song['added_at'])\n",
    "    song_release_date.append(song['track']['album']['release_date'])\n",
    "    all_artists = song['track']['artists']\n",
    "    artists = []\n",
    "    for a in all_artists:\n",
    "        artists.append(a['name'])\n",
    "    artists_col.append(artists)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataframe and extracting the song features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combining the data\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'name':song_name,\n",
    "    'popularity':song_popu,\n",
    "    'date_added':pd.to_datetime(song_added_date),\n",
    "    'release_year':list(map(lambda x: int(x[:4]), song_release_date)),\n",
    "    'artists':artists_col\n",
    "    })\n",
    "\n",
    "#audio features\n",
    "\n",
    "features = sp.audio_features(song_id)\n",
    "\n",
    "feat_names = list(sp.audio_features(song_id)[0].keys())\n",
    "\n",
    "for row in range(len(features)):\n",
    "    for col in range(len(feat_names)):\n",
    "        df.loc[row, feat_names[col]] = features[row][feat_names[col]]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recency Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_month = datetime.today().month\n",
    "curr_year = datetime.today().year\n",
    "\n",
    "recency = list(map(lambda x: curr_month - x.month if (x.year == curr_year) else curr_month + (12 - x.month)\n",
    "                    + (curr_year - x.year - 1) * 12, df['date_added']))\n",
    "df['recency'] = recency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing Popularity before ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popu ranges between 0 to 100, so normalizing it to 0 to 20\n",
    "df['popularity'] = list(map(lambda x: x // 5, df['popularity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting the rows whereever year is null\n",
    "for i in range(len(df['release_year'])):\n",
    "    if df.loc[i, 'release_year'] == 0:\n",
    "        df.drop(i, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OHE on Popularity and Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cosin similarity, we need the size of the vectors to be same, so we are genralizing the columns\n",
    "\n",
    "for i in range(0,21):\n",
    "    df[f\"popu|{i}\"] = [0] * len(df['name'])\n",
    "\n",
    "for i in range(1980, datetime.today().year + 1):\n",
    "    df[f\"year|{i}\"] = [0] * len(df['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will create dataframe with the columns of unique values in the series\n",
    "df_year = pd.get_dummies(df['release_year'])\n",
    "df_popu = pd.get_dummies(df['popularity'])\n",
    "\n",
    "# assigning names to the columns\n",
    "df_year.columns = map(lambda x: 'year' + '|' + str(x), df_year.columns)\n",
    "df_popu.columns = map(lambda x: 'popu' + '|' + str(x), df_popu.columns)\n",
    "# df_popu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now updating the columns with values wherever needed\n",
    "\n",
    "for col in df_popu.columns:\n",
    "    df[col] = df_popu[col]\n",
    "for col in df_year.columns:\n",
    "    df[col] = df_year[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[:20, 24:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OHE on Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file contains artists names which will be used for ohe\n",
    "artists_excel = pd.read_excel('datasets/artists_names.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy dataframe for ohe-ing the artists\n",
    "zeros = [0] * len(df['name'])\n",
    "extra = pd.DataFrame(zeros)\n",
    "for name in artists_excel['artists']:\n",
    "    extra[f\"artist|{name}\"] = 0\n",
    "\n",
    "new_df = pd.concat([df, extra], axis=1)\n",
    "new_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# to place 1 whenever the artist in row cell matches with the column artist\n",
    "for i, row in new_df.iterrows():\n",
    "    for name in row['artists']:\n",
    "        if name in list(artists_excel['artists']):\n",
    "            new_df.loc[i, f\"artist|{name}\"] = 1\n",
    "\n",
    "new_df = new_df.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.iloc[:20, 91:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# new df for generating the recommedation vector\n",
    "# we are dropping the non-integer columns as they are of no use in calulating the similarity\n",
    "'''\n",
    "recomm_vec_df = new_df.drop(['name', 'popularity', 'date_added', 'release_year', 'type', 'id', 'uri', 'track_href',  'analysis_url', 'artists'], axis=1)\n",
    "# recomm_vec_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "now calculating the bias which are going to be multiplied with each of the rows individually.\n",
    "for that we need to understand this that the bias must reduce the values of the older songs, so we need bias\n",
    "to be between 0 and 1\n",
    "1 / recency is not working as it is drastically reducing the values whih can negatively impact the recommendations\n",
    "0.9 ** recency might work. For recency 3, we get the weight as 0.729, which is totally fine as we tend to listen less\n",
    "to songs which are older than 3 months. Also, going below this value can trigger false recommendations\n",
    "through this we are actually reducing the effect of older (added) songs\n",
    "'''\n",
    "\n",
    "recomm_vec_df['bias'] = list(map(lambda x: round(0.9 ** x, 5), list(recomm_vec_df['recency'])))\n",
    "for col in recomm_vec_df.columns:\n",
    "    if (col != 'bias' or col != 'recency'):\n",
    "        recomm_vec_df[col] = recomm_vec_df[col] * recomm_vec_df['bias']\n",
    "# recomm_vec_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(recomm_vec_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting the bias and recency columns\n",
    "recomm_vec_df = recomm_vec_df.dropna().drop(['bias', 'recency', 'key', 'mode', 'duration_ms', 'time_signature'], axis=1)\n",
    "recomm_vec_df['tempo'] = recomm_vec_df['tempo'].apply(lambda x: (x - min(recomm_vec_df['tempo'])) / (max(recomm_vec_df['tempo'] - min(recomm_vec_df['tempo']))))\n",
    "recomm_vec_df['loudness'] = recomm_vec_df['loudness'].apply(lambda x: (x - min(recomm_vec_df['loudness'])) / (max(recomm_vec_df['loudness'] - min(recomm_vec_df['loudness']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recomm_vec_df.head(9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the Recommendation vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one will create the song features columns vector\n",
    "recomm_vec1 = np.array(list(map(lambda col: recomm_vec_df[col].mean(), recomm_vec_df.loc[:, :\"tempo\"].columns)))\n",
    "# this one will create the ohe columns till current year vector\n",
    "recomm_vec2 = np.array(list(map(lambda col: sum(recomm_vec_df[col]), recomm_vec_df.loc[:, \"popu|0\":f\"year|{datetime.today().year}\"].columns)))\n",
    "# artists only ohe columns vector\n",
    "recomm_vec3 = np.array(list(map(lambda col: sum(recomm_vec_df[col]), recomm_vec_df.iloc[:, -len(artists_excel['artists']):].columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotion code\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    result = DeepFace.analyze(img_path = frame , actions=['emotion'], enforce_detection=False )\n",
    "\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray,1.1,4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 3)\n",
    "\n",
    "    emotion = result[0]['dominant_emotion']\n",
    "\n",
    "    txt = str(emotion)\n",
    "\n",
    "    cv2.putText(frame, txt, (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 3)\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# print(emotion)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#output: emotion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the pre-processed dataset containing the 1000s of songs\n",
    "data = pd.read_csv('datasets/final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns which will be used for the filtering\n",
    "filt_col = ['acousticness', 'danceability', 'energy', 'loudness', 'tempo', 'valence']\n",
    "\n",
    "# values for filtering (Emotion specific)\n",
    "happy_low = [0, 0.57, 0.4, -10.4, 75 ,0.25]\n",
    "sad_low = [0.2, 0.3, 0.25, -11, 70, 0]\n",
    "chill_low = [0, 0.35, 0.25, -12.7, 80, 0.2]\n",
    "angry_low = [0, 0.46, 0.56, -11, 90, 0.2]\n",
    "\n",
    "happy_high = [0.75, 0.86, 1, -3, 170, 1]\n",
    "sad_high = [0.9, 0.7, 0.8, -4, 160, 0.7]\n",
    "chill_high = [0.85, 0.8, 0.8, -4, 165, 0.9]\n",
    "angry_high = [0.6, 0.85, 1, -4, 170, 0.75]\n",
    "\n",
    "happy_avg = [0.715, 0.7, 0.375, -6.7, 0.625, 123]\n",
    "sad_avg = [0.5, 0.525, 0.55, -7.5, 0.3, 115]\n",
    "chill_avg = [0.575, 0.525, 0.425, -8.35, 0.55, 122.5]\n",
    "angry_avg = [0.655, 0.78, 0.3, -7.5, 0.475, 130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "if emotion == 'happy':\n",
    "    for col in filt_col:\n",
    "        data = data[(data[col] > happy_low[i]) & (data[col] < happy_high[i])]\n",
    "        i += 1\n",
    "\n",
    "    sim = []\n",
    "    for i in range(len(data)):\n",
    "        e = data.loc[:, filt_col].iloc[i].values\n",
    "        sim.append(np.linalg.norm(e - happy_avg)/70)\n",
    "    data['sim'] = (np.array(sim) - max(sim)) * (-1)\n",
    "    print(data['sim'])\n",
    "\n",
    "elif emotion == 'sad':\n",
    "    for col in filt_col:\n",
    "        data = data[(data[col] > sad_low[i]) & (data[col] < sad_high[i])]\n",
    "        i += 1\n",
    "        \n",
    "    sim = []\n",
    "    for i in range(len(data)):\n",
    "        e = data.loc[:, filt_col].iloc[i].values\n",
    "        sim.append(np.linalg.norm(e - sad_avg)/70)\n",
    "    data['sim'] = (np.array(sim) - max(sim)) * (-1)\n",
    "    print(data['sim'])\n",
    "\n",
    "elif emotion == 'neutral':\n",
    "    for col in filt_col:\n",
    "        data = data[(data[col] > chill_low[i]) & (data[col] < chill_high[i])]\n",
    "        i += 1\n",
    "    \n",
    "    sim = []\n",
    "    for i in range(len(data)):\n",
    "        e = data.loc[:, filt_col].iloc[i].values\n",
    "        sim.append(np.linalg.norm(e - chill_avg)/70)\n",
    "    data['sim'] = (np.array(sim) - max(sim)) * (-1)\n",
    "    print(data['sim'])\n",
    "\n",
    "elif emotion == 'angry':\n",
    "    for col in filt_col:\n",
    "        data = data[(data[col] > angry_low[i]) & (data[col] < angry_high[i])]\n",
    "        i += 1\n",
    "\n",
    "    sim = []\n",
    "    for i in range(len(data)):\n",
    "        e = data.loc[:, filt_col].iloc[i].values\n",
    "        sim.append(np.linalg.norm(e - angry_avg)/70)\n",
    "    data['sim'] = (np.array(sim) - max(sim)) * (-1)\n",
    "    print(data['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data.drop(['name', 'popularity', 'date_added', 'release_year', 'type', 'id', 'uri', 'track_href',  'analysis_url', 'artists', 'Unnamed: 0', 'key', 'mode', 'duration_ms', 'time_signature'], axis=1)\n",
    "# data_filtered.drop(0, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered['tempo'] = data_filtered['tempo'].apply(lambda x: (x - min(data_filtered['tempo'])) / (max(data_filtered['tempo'] - min(data_filtered['tempo']))))\n",
    "data_filtered['loudness'] = data_filtered['loudness'].apply(lambda x: (x - min(data_filtered['loudness'])) / (max(data_filtered['loudness'] - min(data_filtered['loudness']))))\n",
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.iloc[:20, -1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the Vectors/Rows Using Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Using Euclidian Distance as both magnitude and directions are important\n",
    "Euclidean distance measures the distance between two points in a multidimensional space by calculating the square\n",
    "root of the sum of the squared differences between their corresponding elements. It is suitable for continuous data\n",
    "where the magnitude and direction of each feature are important.\n",
    "'''\n",
    "\n",
    "l1 = []\n",
    "l2 = []\n",
    "l3 = []\n",
    "s = 0\n",
    "recommendations = pd.DataFrame({'name': data['name'], 'artists':data['artists'], 'id': data['id'], 'sim': data['sim']})\n",
    "for i in range(len(data_filtered)):\n",
    "    # this contains the columns from start till the ohe\n",
    "    data_1 = data_filtered.loc[:, :\"tempo\"].iloc[i].values\n",
    "    # this contains the ohe columns till current year\n",
    "    data_2 = data_filtered.loc[:, \"popu|0\":f\"year|{datetime.today().year}\"].iloc[i].values\n",
    "    # this contains the artists only columns\n",
    "    data_3 = data_filtered.iloc[:, (-len(artists_excel['artists']) - 1):-1].iloc[i].values\n",
    "\n",
    "    sim1 = np.linalg.norm(recomm_vec1 - data_1)  # euclidian distance\n",
    "    '''\n",
    "    we are getting a dissimilarity score, as greater the difference \n",
    "    between the values, higher would be the score. The values which differ largerly with respect to the vector\n",
    "    will tend to have a higher eucladian score\n",
    "    '''\n",
    "\n",
    "    # simply using dot product\n",
    "    sim2 = np.dot(recomm_vec2, data_2)\n",
    "\n",
    "    sim3 = np.dot(recomm_vec3, data_3)\n",
    "    \n",
    "    l1.append(round(sim1, 6))\n",
    "    l2.append(round(sim2, 6))\n",
    "    l3.append(round(sim3, 6))\n",
    "\n",
    "l1 = (np.array(l1) - max(l1)) * (-1)  # converting it into a similarity score\n",
    "\n",
    "# normalizing the array values to 0-1 range for proper contribution in the recommendation\n",
    "l2 = (np.array(l2) - min(l2)) / (max(l2) - min(l2))\n",
    "l3 = (np.array(l3) - min(l3)) / (max(l3) - min(l3)) * 0.5\n",
    "\n",
    "score = l1 + l2 + l3\n",
    "# print(type(recommendations['sim'][0]))\n",
    "# print(type(recommendations['sim']), type(pd.Series(score)))\n",
    "recommendations['sim'] = recommendations['sim'] + score  # as sim col is already filled with emotion effiency score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_filtered.iloc[:, (-len(artists_excel['artists']) - 1):-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations.drop_duplicates(['id'], inplace=True)\n",
    "\n",
    "# sorting the recommendations\n",
    "recommendations = recommendations.sort_values(['sim'], axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommendations.reset_index().drop('index', axis=1)\n",
    "print(\"No. of Song Recommendations: \", len(recommendations))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 50 recommendations\n",
    "recommendations.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for playing the songs but requires premium account\n",
    "# sp.start_playback(uris=['spotify:track:1stiSonuKkZqhI1o9nZ9MT'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END ðŸ™‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
